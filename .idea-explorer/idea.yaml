idea:
  title: How to constrain LLM's behavior?
  domain: artificial_intelligence
  hypothesis: 'It is possible to motivate large language models (LLMs) to add constraints
    to their behavior such that, instead of generating hallucinated responses, they
    can choose to abstain from answering, say "I don''t know," or indicate that more
    data is needed, and by choosing this option, they are better off. Creating a training
    signal or verifier model can help LLMs recognize when they are incapable of answering
    a question.

    '
  background:
    description: While humans are very good at acting selectively, this is fundamentally
      hard for LLMs to do as they're generative models. The research question is how
      to motivate LLMs to add constraints to their behavior so that, instead of generating
      hallucinated responses, they have the option to abstain, say "I don't know,"
      or "I need more data," and by choosing this option, they're better off. The
      challenge is to create a training signal or a verifier model that makes LLMs
      realize at one moment that they are incapable of answering a question.
  metadata:
    source: IdeaHub
    source_url: https://hypogenic.ai/ideahub/idea/nZ3Wq17WoqBQcUuNVU8e
    idea_id: how_to_constrain_llm_s_behavio_20251228_233521_db86175d
    created_at: '2025-12-28T23:35:21.517430'
    status: submitted
    github_repo_name: constrain-llm-behavior-gemini
    github_repo_url: https://github.com/Hypogenic-AI/constrain-llm-behavior-gemini
