@article{2405.01563,
  title={Mitigating LLM Hallucinations via Conformal Abstention},
  author={Zhu, Quanquan and others},
  journal={arXiv preprint arXiv:2405.01563},
  year={2024}
}

@article{2404.10960,
  title={Uncertainty-Based Abstention in LLMs Improves Safety and Reduces Hallucinations},
  author={Mielke, Sabrina J. and others},
  journal={arXiv preprint arXiv:2404.10960},
  year={2024}
}

@article{2407.18418,
  title={Know Your Limits: A Survey of Abstention in Large Language Models},
  author={Zhang, Miao and others},
  journal={arXiv preprint arXiv:2407.18418},
  year={2024}
}

@article{2402.00367,
  title={Don't Hallucinate, Abstain: Identifying LLM Knowledge Gaps via Multi-LLM Collaboration},
  author={Li, Shangbin and others},
  journal={arXiv preprint arXiv:2402.00367},
  year={2024}
}

@article{2303.08896,
  title={SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models},
  author={Manakul, Potsawee and Liusie, Adian and Gales, Mark J. F.},
  journal={arXiv preprint arXiv:2303.08896},
  year={2023}
}
